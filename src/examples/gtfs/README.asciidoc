= Example pipeline to load a GTFS feed

This directory contains an example pipeline to load https://gtfs.org/documentation/schedule/reference/[GTFS] feed data
into Neo4j. The data model is re-used from the https://faboo.org/2021/01/loading-uk-gtfs/[Loading the UK GTFS data feed]
blog post, rewriting the `LOAD CSV` used there to build a pipeline as a showcase.

To run it, a GTFS feed needs to be placed into an directory.

The `gtfs.py` is mostly about setting up an cli and loading/reading arguments and env variables.

The actual pipeline is constructed via:

[source,python]
----
context = ETLContext(env_vars=dict(os.environ))

schema = SchemaTask(context=context)
init_group = TaskGroup(context=context, tasks=[schema], name="schema-init")

tasks = [
    LoadAgenciesTask(context=context, file=input_directory / LoadAgenciesTask.file_name()),
    LoadRoutesTask(context=context, file=input_directory / LoadRoutesTask.file_name()),
    LoadStopsTask(context=context, file=input_directory / LoadStopsTask.file_name()),
    LoadTripsTask(context=context, file=input_directory / LoadTripsTask.file_name()),
    LoadCalendarTask(context=context, file=input_directory / LoadCalendarTask.file_name()),
    LoadStopTimesTask(context=context, file=input_directory / LoadStopTimesTask.file_name()),
]
csv_group = TaskGroup(context=context, tasks=tasks, name="csv-loading")

post_group = TaskGroup(context=context, tasks=[CreateSequenceTask(context=context)], name="post-processing")

all_group = TaskGroup(context=context, tasks=[init_group, csv_group, post_group], name="main")

context.reporter.register_tasks(all_group, source=input_directory.name)

all_group.execute()
----

An `all_group` is constructed from the `TaskGroup` `init_group, csv_group, post_group]` which again consists of other Task.

The `all_group.execute()` triggers the pipeline. The Tasks execution follows the order of the task in the arrays.

Validating the input data is done via Pydantic. See `Load*Task.py` for examples. Lines not matching the Pydantic rules are written as `*.error.json` per input file and can be examined after execution.

Tasks have the ability to abort the entire pipeline by implementing the `abort_on_fail()` function and return `True`.
