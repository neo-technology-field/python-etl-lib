= Python ETL Toolbox

Is this yet another Python ETL tool? Perhaps, but the purpose here is not to offer a ready-made solution. Instead, this library provides a collection of building blocks designed to help developers quickly assemble an ETL pipeline.

The included components offer built-in functionality deemed essential for ETL pipelines, such as:

* logging
* error handling
* validation (via Pydantic)
* batching and streaming

While this library currently focuses on Neo4j databases, it can be extended to other sources and sinks as needed. It does not provide a CLI out of the box, but the example usage offers inspiration for customization.

== Main Building Blocks

=== Task

Pipelines can be built as a series of Tasks. For example, loading a CSV file into Neo4j can be implemented using `CSVLoad2Neo4jTask` or `ExecuteCypherTask`. Tasks do not pass data directly between one another; instead, the `execute()` function provides status information to the caller.

Tasks integrate seamlessly with the `ProgressReporter`, which can optionally log status information to a Neo4j database.

To maximize flexibility, Tasks can be composed of `BatchProcessor`s, allowing developers to quickly assemble Tasks from existing building blocks.

=== TaskGroups

Tasks can be grouped into logical blocks, such as init, loading, and post-processing. The provided functionality is designed to simplify the composition of ETL pipelines while supporting logging, error handling, and other key features.

=== BatchProcessors

``BatchProcessor``s allow the creation of Tasks by chaining smaller components, such as reading data from a source, validating it, and writing it to a sink—all while processing data in batches. Refer to `CSVLoad2Neo4jTask` for an example implementation.

== ETLContext

The ETLContext object holds shared information and functionality required by all components of the pipeline. It also contains a `Neo4jContext` for Neo4j-specific operations.

=== ProgressReporter

The `ProgressReporter` is responsible for logging Task progress as the pipeline executes. If the `REPORTER_DATABASE` environment variable is set, the reporter will log progress information to a Neo4j database.

A NeoDash dashboard is provided to visualize information about previous ETL runs.

Each Task provides details such as rows read, nodes created, and nodes deleted. Only non-zero statistics are logged through Python's logging module. When reporting to a database, all statistics are included.

Logging is implemented using Python's standard logging package. Users are responsible for configuring logging as required. The GTFS example demonstrates logging to both a file and the console, with most messages logged at the INFO level.

Example output from the GTFS example is shown below:

[source,python,options="nowrap"]
----
2025-01-06 18:26:03,507 - INFO - Processing directory: /Users/bert/Downloads/mdb-2333-202412230030
2025-01-06 18:26:03,507 - INFO - Neo4j URL: neo4j://localhost:7687
2025-01-06 18:26:03,507 - INFO - Neo4j User: neo4j
2025-01-06 18:26:03,507 - INFO - Neo4j Database Name: neo4j
2025-01-06 18:26:03,507 - INFO - Connecting to Neo4j at neo4j://localhost:7687 with user neo4j to access database neo4j...
2025-01-06 18:26:03,522 - INFO - driver connected to instance at neo4j://localhost:7687 with username neo4j
2025-01-06 18:26:03,522 - INFO - progress reporting to database: neo4j
2025-01-06 18:26:03,524 - INFO - the following tasks are registered for execution:
Node('/main', task=TaskGroup(main))
├── Node('/main/schema-init', task=TaskGroup(schema-init))
│   └── Node('/main/schema-init/SchemaTask', task=Task(SchemaTask))
├── Node('/main/csv-loading', task=TaskGroup(csv-loading))
│   ├── Node('/main/csv-loading/LoadAgenciesTask', task=LoadAgenciesTask(mdb-2333-202412230030/agency.txt))
│   ├── Node('/main/csv-loading/LoadRoutesTask', task=LoadRoutesTask(mdb-2333-202412230030/routes.txt))
│   ├── Node('/main/csv-loading/LoadStopsTask', task=LoadStopsTask(mdb-2333-202412230030/stops.txt))
│   ├── Node('/main/csv-loading/LoadTripsTask', task=LoadTripsTask(mdb-2333-202412230030/trips.txt))
│   ├── Node('/main/csv-loading/LoadCalendarTask', task=LoadCalendarTask(mdb-2333-202412230030/calendar.txt))
│   └── Node('/main/csv-loading/LoadStopTimesTask', task=LoadStopTimesTask(mdb-2333-202412230030/stop_times.txt))
└── Node('/main/post-processing', task=TaskGroup(post-processing))
    └── Node('/main/post-processing/CreateSequenceTask', task=Task(CreateSequenceTask))
2025-01-06 18:26:03,568 - INFO - starting main
2025-01-06 18:26:03,585 - INFO - 	starting schema-init
2025-01-06 18:26:03,587 - INFO - 		starting SchemaTask
2025-01-06 18:26:03,595 - INFO - 		finished SchemaTask with success: True
2025-01-06 18:26:03,609 - INFO - 	finished schema-init with success: True
2025-01-06 18:26:03,611 - INFO - 	starting csv-loading
2025-01-06 18:26:03,612 - INFO - 		starting LoadAgenciesTask
2025-01-06 18:26:03,624 - INFO - 		finished LoadAgenciesTask with success: True
+------------------+------------------+--------------+
|   csv_lines_read |   properties_set |   valid_rows |
|------------------+------------------+--------------|
|                1 |                4 |            1 |
+------------------+------------------+--------------+
2025-01-06 18:26:03,626 - INFO - 		starting LoadRoutesTask
2025-01-06 18:26:03,638 - INFO - 		finished LoadRoutesTask with success: True
+------------------+------------------+--------------+
|   csv_lines_read |   properties_set |   valid_rows |
|------------------+------------------+--------------|
|              299 |              897 |          299 |
+------------------+------------------+--------------+
2025-01-06 18:26:03,640 - INFO - 		starting LoadStopsTask
2025-01-06 18:26:03,763 - INFO - 		finished LoadStopsTask with success: True
+------------------+------------------+--------------+
|   csv_lines_read |   properties_set |   valid_rows |
|------------------+------------------+--------------|
|             4170 |            20850 |         4170 |
+------------------+------------------+--------------+
2025-01-06 18:26:03,765 - INFO - 		starting LoadTripsTask
2025-01-06 18:26:05,932 - INFO - 		finished LoadTripsTask with success: False, error:
Randomly thrown exception!
2025-01-06 18:26:05,948 - WARNING - Task csv-loading failed. Aborting execution.
2025-01-06 18:26:05,949 - INFO - 	finished csv-loading with success: False
+------------------+------------------+--------------+
|   csv_lines_read |   properties_set |   valid_rows |
|------------------+------------------+--------------|
|             4470 |            21751 |         4470 |
+------------------+------------------+--------------+
2025-01-06 18:26:05,950 - WARNING - Task main failed. Aborting execution.
2025-01-06 18:26:05,950 - INFO - finished main with success: False
+------------------+------------------+--------------+
|   csv_lines_read |   properties_set |   valid_rows |
|------------------+------------------+--------------|
|             4470 |            21751 |         4470 |
+------------------+------------------+--------------+
2025-01-06 18:26:05,952 - INFO - Processing complete.
----

When reporting to a Neo4j database, each ETL run results in a tree structure like the one shown below (example from the GTFS example):

image::documentation/schema.png[Schema]

Each ETLTask node, once the associated task has been completed, will have an attached `ETLStats` node with properties such as below. The stats reported here depend on the task(s) involved.

[code]
----
csv_lines_read:4170,
labels_removed:0,
indexes_removed:0,
constraints_added:0,
relationships_created:0,
nodes_deleted:0,
indexes_added:0,
relationships_deleted:0,
properties_set:20850,
invalid_rows:0,
constraints_removed:0,
labels_added:0,
nodes_created:0,
valid_rows:4170
----

Tasks with SubTasks (defined as `TaskGroup`) aggregate statistics from all their child Tasks. Therefore, viewing the top-level `ETLRun` node provides a summary of the entire pipeline.

A simple NeoDash dashboard configuration is provided in the `dashboard.json` file. For more information, visit the https://neo4j.com/labs/neodash/[NeoDash documentation].

== Building, Testing, Running

This project uses https://realpython.com/pipenv-guide/[Pipenv].

To set up, activate the environment with `pipenv shell` and then `run pipenv install` to install all dependencies.

Run the GTFS example using the following command:
----
pipenv run src/examples/gtfs/gtfs.py <gtfs input directory>
----

=== Tests

Most tests need a Neo4j database. 2 options exists:

. Use en existing running database. Provide the following env variables:
* `NEO4J_URI`
* `NEO4J_USERNAME`
* `NEO4J_PASSWORD`
* `NEO4J_TEST_DATABASE`
. Use testcontainers to start and stop a Neo4j database.
This option is activated when the env variable `NEO4J_TEST_CONTAINER` is detected. This variable determines which docker image to run. In this case, the variables from option1 are ignored.

Run the tests via `pipenv run pytest`.

