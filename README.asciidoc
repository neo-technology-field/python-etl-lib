= Python ETL Toolbox

Is this yet another Python ETL tool? Perhaps, but the purpose here is not to offer a ready-made solution. Instead, this library provides a collection of building blocks designed to help developers quickly assemble an ETL pipeline.

The included components offer built-in functionality deemed essential for ETL pipelines, such as:

* logging
* error handling
* validation (via Pydantic)
* batching and streaming

While this library currently focuses on Neo4j databases, it can be extended to other sources and sinks as needed. It does not provide a CLI out of the box, but the example usage offers inspiration for customization.

== Main Building Blocks

=== Task

Pipelines can be built as a series of Tasks. For example, loading a CSV file into Neo4j can be implemented using `CSVLoad2Neo4jTask` or `ExecuteCypherTask`. Tasks do not pass data directly between one another; instead, the `execute()` function provides status information to the caller.

Tasks integrate seamlessly with the `ProgressReporter`, which can optionally log status information to a Neo4j database.

To maximize flexibility, Tasks can be composed of `BatchProcessor`s, allowing developers to quickly assemble Tasks from existing building blocks.

=== TaskGroups

Tasks can be grouped into logical blocks, such as init, loading, and post-processing. The provided functionality is designed to simplify the composition of ETL pipelines while supporting logging, error handling, and other key features.

=== BatchProcessors

``BatchProcessor``s allow the creation of Tasks by chaining smaller components, such as reading data from a source, validating it, and writing it to a sink—all while processing data in batches. Refer to `CSVLoad2Neo4jTask` for an example implementation.

== ETLContext

The ETLContext object holds shared information and functionality required by all components of the pipeline. It also contains a `Neo4jContext` for Neo4j-specific operations.

=== ProgressReporter

The `ProgressReporter` is responsible for logging Task progress as the pipeline executes. If the `REPORTER_DATABASE` environment variable is set, the reporter will log progress information to a Neo4j database.

A NeoDash dashboard is provided to visualize information about previous ETL runs.

Each Task provides details such as rows read, nodes created, and nodes deleted. Only non-zero statistics are logged through Python's logging module. When reporting to a database, all statistics are included.

Logging is implemented using Python's standard logging package. Users are responsible for configuring logging as required. The GTFS example demonstrates logging to both a file and the console, with most messages logged at the INFO level.

Example output from the GTFS example is shown below:

[source,python,options="nowrap"]
----
025-01-18 18:44:45,214 - INFO - Processing directory: /Users/bert/Downloads/mdb-2333-202412230030
2025-01-18 18:44:45,214 - INFO - Neo4j URL: neo4j://localhost:7687
2025-01-18 18:44:45,214 - INFO - Neo4j User: neo4j
2025-01-18 18:44:45,214 - INFO - Neo4j Database Name: neo4j
2025-01-18 18:44:45,214 - INFO - Connecting to Neo4j at neo4j://localhost:7687 with user neo4j to access database neo4j...
2025-01-18 18:44:45,239 - INFO - driver connected to instance at neo4j://localhost:7687 with username neo4j and database neo4j
2025-01-18 18:44:45,239 - INFO - progress reporting to database: neo4j
└──main
   ├──schema-init
   │  └──SchemaTask
   ├──csv-loading
   │  ├──LoadAgenciesTask('/Users/bert/Downloads/mdb-2333-202412230030/agency.txt')
   │  ├──LoadRoutesTask('/Users/bert/Downloads/mdb-2333-202412230030/routes.txt')
   │  ├──LoadStopsTask('/Users/bert/Downloads/mdb-2333-202412230030/stops.txt')
   │  ├──LoadTripsTask('/Users/bert/Downloads/mdb-2333-202412230030/trips.txt')
   │  ├──LoadCalendarTask('/Users/bert/Downloads/mdb-2333-202412230030/calendar.txt')
   │  └──LoadStopTimesTask('/Users/bert/Downloads/mdb-2333-202412230030/stop_times.txt')
   └──post-processing
      └──CreateSequenceTask

2025-01-18 18:44:45,715 - INFO - starting main
2025-01-18 18:44:45,752 - INFO - starting schema-init
2025-01-18 18:44:45,757 - INFO - starting SchemaTask
2025-01-18 18:44:45,871 - INFO - finished SchemaTask with success: True
+-----------------+---------------------+
|   indexes_added |   constraints_added |
|-----------------+---------------------|
|               3 |                   4 |
+-----------------+---------------------+
2025-01-18 18:44:46,017 - INFO - finished schema-init with success: True
+---------------------+-----------------+
|   constraints_added |   indexes_added |
|---------------------+-----------------|
|                   4 |               3 |
+---------------------+-----------------+
2025-01-18 18:44:46,024 - INFO - starting csv-loading
2025-01-18 18:44:46,048 - INFO - starting LoadAgenciesTask('/Users/bert/Downloads/mdb-2333-202412230030/agency.txt')
2025-01-18 18:44:46,178 - INFO - finished LoadAgenciesTask('/Users/bert/Downloads/mdb-2333-202412230030/agency.txt') with success: True
+----------------+-----------------+------------------+--------------+------------------+
|   labels_added |   nodes_created |   properties_set |   valid_rows |   csv_lines_read |
|----------------+-----------------+------------------+--------------+------------------|
|              1 |               1 |                5 |            1 |                1 |
+----------------+-----------------+------------------+--------------+------------------+
2025-01-18 18:44:46,184 - INFO - starting LoadRoutesTask('/Users/bert/Downloads/mdb-2333-202412230030/routes.txt')
2025-01-18 18:44:46,417 - INFO - finished LoadRoutesTask('/Users/bert/Downloads/mdb-2333-202412230030/routes.txt') with success: True
+----------------+-----------------+------------------+--------------+------------------+-------------------------+
|   labels_added |   nodes_created |   properties_set |   valid_rows |   csv_lines_read |   relationships_created |
|----------------+-----------------+------------------+--------------+------------------+-------------------------|
|            299 |             299 |             1196 |          299 |              299 |                     299 |
+----------------+-----------------+------------------+--------------+------------------+-------------------------+
2025-01-18 18:44:46,423 - INFO - starting LoadStopsTask('/Users/bert/Downloads/mdb-2333-202412230030/stops.txt')
2025-01-18 18:44:46,870 - INFO - finished LoadStopsTask('/Users/bert/Downloads/mdb-2333-202412230030/stops.txt') with success: True
+----------------+-----------------+------------------+--------------+------------------+
|   labels_added |   nodes_created |   properties_set |   valid_rows |   csv_lines_read |
|----------------+-----------------+------------------+--------------+------------------|
|           4170 |            4170 |            25019 |         4170 |             4170 |
+----------------+-----------------+------------------+--------------+------------------+
2025-01-18 18:44:46,875 - INFO - starting LoadTripsTask('/Users/bert/Downloads/mdb-2333-202412230030/trips.txt')
2025-01-18 18:44:51,782 - INFO - finished LoadTripsTask('/Users/bert/Downloads/mdb-2333-202412230030/trips.txt') with success: True
+------------------+-------------------------+----------------+-----------------+--------------+------------------+
|   properties_set |   relationships_created |   labels_added |   nodes_created |   valid_rows |   csv_lines_read |
|------------------+-------------------------+----------------+-----------------+--------------+------------------|
|           614069 |                   91694 |          91694 |           91694 |        91694 |            91694 |
+------------------+-------------------------+----------------+-----------------+--------------+------------------+
2025-01-18 18:44:51,786 - INFO - starting LoadCalendarTask('/Users/bert/Downloads/mdb-2333-202412230030/calendar.txt')
2025-01-18 18:44:52,262 - INFO - finished LoadCalendarTask('/Users/bert/Downloads/mdb-2333-202412230030/calendar.txt') with success: True
+----------------+--------------+------------------+
|   labels_added |   valid_rows |   csv_lines_read |
|----------------+--------------+------------------|
|         198095 |          212 |              212 |
+----------------+--------------+------------------+
2025-01-18 18:44:52,266 - INFO - starting LoadStopTimesTask('/Users/bert/Downloads/mdb-2333-202412230030/stop_times.txt')
2025-01-18 18:46:22,633 - INFO - finished LoadStopTimesTask('/Users/bert/Downloads/mdb-2333-202412230030/stop_times.txt') with success: True
+------------------+-------------------------+----------------+-----------------+--------------+------------------+
|   properties_set |   relationships_created |   labels_added |   nodes_created |   valid_rows |   csv_lines_read |
|------------------+-------------------------+----------------+-----------------+--------------+------------------|
|          9494080 |                 3797632 |        1898816 |         1898816 |      1898816 |          1898816 |
+------------------+-------------------------+----------------+-----------------+--------------+------------------+
2025-01-18 18:46:22,653 - INFO - finished csv-loading with success: True
+------------------+-------------------------+----------------+-----------------+--------------+------------------+
|   properties_set |   relationships_created |   labels_added |   nodes_created |   valid_rows |   csv_lines_read |
|------------------+-------------------------+----------------+-----------------+--------------+------------------|
|         10134369 |                 3889625 |        2193075 |         1994980 |      1995192 |          1995192 |
+------------------+-------------------------+----------------+-----------------+--------------+------------------+
2025-01-18 18:46:22,655 - INFO - starting post-processing
2025-01-18 18:46:22,668 - INFO - starting CreateSequenceTask
2025-01-18 18:46:32,888 - INFO - finished CreateSequenceTask with success: True
2025-01-18 18:46:32,892 - INFO - finished post-processing with success: True
2025-01-18 18:46:32,894 - INFO - finished main with success: True
+------------------+-----------------+-------------------------+----------------+-----------------+---------------------+--------------+------------------+
|   properties_set |   indexes_added |   relationships_created |   labels_added |   nodes_created |   constraints_added |   valid_rows |   csv_lines_read |
|------------------+-----------------+-------------------------+----------------+-----------------+---------------------+--------------+------------------|
|         10134369 |               3 |                 3889625 |        2193075 |         1994980 |                   4 |      1995192 |          1995192 |
+------------------+-----------------+-------------------------+----------------+-----------------+---------------------+--------------+------------------+
2025-01-18 18:46:32,897 - INFO - Processing complete.
----

When reporting to a Neo4j database, each ETL run results in a tree structure like the one shown below (example from the GTFS example):

image::documentation/schema.png[Schema]

Each ETLTask node, once the associated task has been completed, will have an attached `ETLStats` node with properties such as below. The stats reported here depend on the task(s) involved.

[code]
----
csv_lines_read:4170,
labels_removed:0,
indexes_removed:0,
constraints_added:0,
relationships_created:0,
nodes_deleted:0,
indexes_added:0,
relationships_deleted:0,
properties_set:20850,
invalid_rows:0,
constraints_removed:0,
labels_added:0,
nodes_created:0,
valid_rows:4170
----

Tasks with SubTasks (defined as `TaskGroup`) aggregate statistics from all their child Tasks. Therefore, viewing the top-level `ETLRun` node provides a summary of the entire pipeline.

A simple NeoDash dashboard configuration is provided in the `dashboard.json` file. For more information, visit the https://neo4j.com/labs/neodash/[NeoDash documentation].

== Building, Testing, Running

This project uses https://realpython.com/pipenv-guide/[Pipenv].

To set up, activate the environment with `pipenv shell` and then `run pipenv install` to install all dependencies.

Run the GTFS example using the following command:
----
pipenv run src/examples/gtfs/gtfs.py <gtfs input directory>
----

=== Tests

Most tests need a Neo4j database. 2 options exists:

. Use en existing running database. Provide the following env variables:
* `NEO4J_URI`
* `NEO4J_USERNAME`
* `NEO4J_PASSWORD`
* `NEO4J_TEST_DATABASE`
. Use testcontainers to start and stop a Neo4j database.
This option is activated when the env variable `NEO4J_TEST_CONTAINER` is detected. This variable determines which docker image to run. In this case, the variables from option1 are ignored.

Run the tests via `pipenv run pytest`.

